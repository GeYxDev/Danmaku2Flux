from .database import db
from .services import CrawlerService, model_service


class RecommendationPipeline:
    def __init__(self, bvid):
        self.bvid = bvid
        # ä¸Šä¸‹æ–‡ï¼šç”¨äºåœ¨ç®¡é“å„é˜¶æ®µä¼ é€’æ•°æ®
        self.context = {
            "bvid": bvid,
            "vector": None,  # æ ¸å¿ƒä¸­é—´æ€ï¼šæƒ…æ„Ÿå‘é‡
            "recommendations": []  # æœ€ç»ˆç»“æœ
        }

    def run(self):
        """æ‰§è¡Œç®¡é“æµ"""
        # æ­¥éª¤ 1: æ£€æŸ¥ç¼“å­˜
        self._step_check_database()

        # æ­¥éª¤ 2: (å¦‚æœæ²¡ç¼“å­˜) è·å–æ•°æ® & æ­¥éª¤ 3: è®¡ç®—å‘é‡
        if self.context["vector"] is None:
            self._step_fetch_and_compute()

        # æ­¥éª¤ 4: å‘é‡æœç´¢
        self._step_search()

        return self.context["recommendations"]

    def _step_check_database(self):
        """Stage 1: æŸ¥åº“"""
        cached_vector = db.find_vector_by_bvid(self.bvid)
        if cached_vector:
            print("âš¡ [Pipeline] å‘½ä¸­æ•°æ®åº“ç¼“å­˜ï¼Œè·³è¿‡è®¡ç®—ã€‚")
            self.context["vector"] = cached_vector

    def _step_fetch_and_compute(self):
        """Stage 2 & 3: çˆ¬è™« + æ¨¡å‹"""
        print("ğŸ¢ [Pipeline] æœªå‘½ä¸­ç¼“å­˜ï¼Œå¯åŠ¨å®æ—¶è®¡ç®—æµç¨‹...")
        # 2.1 çˆ¬å–
        danmaku_list = CrawlerService.fetch_danmaku(self.bvid)
        # 2.2 è®¡ç®—
        vector = model_service.predict(danmaku_list)
        self.context["vector"] = vector

        # (å¯é€‰) 2.3: è¿™é‡Œå¯ä»¥æŠŠæ–°è®¡ç®—çš„ç»“æœå­˜å› database.jsonï¼Œå®ç°â€œè¶Šç”¨è¶Šå¿«â€

    def _step_search(self):
        """Stage 4: ç›¸ä¼¼åº¦åŒ¹é…"""
        if self.context["vector"]:
            results = db.search_similar(
                self.context["vector"],
                top_k=5,
                exclude_bvid=self.bvid
            )
            self.context["recommendations"] = results